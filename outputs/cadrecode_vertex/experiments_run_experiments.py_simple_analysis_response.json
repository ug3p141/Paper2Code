[{"choices": [{"message": {"role": "assistant", "content": "# Logic Analysis: experiments/run_experiments.py\n\n## Overview\nThe `experiments/run_experiments.py` module serves as the central orchestrator for all experimental workflows in the CAD-Recode reproduction. This module implements the `ExperimentRunner` class that coordinates training, evaluation, ablation studies, and CAD-QA experiments based on the methodology described in the paper.\n\n## Core Responsibilities\n\n### 1. Experiment Orchestration\n- **Primary Function**: Coordinate all experimental workflows from the paper\n- **Scope**: Training experiments, evaluation on multiple datasets, ablation studies, and CAD-QA tasks\n- **Configuration Management**: Use config.yaml settings to ensure reproducible experiments\n- **Result Collection**: Aggregate and store results from all experimental components\n\n### 2. Workflow Management\n- **Sequential Execution**: Manage dependencies between experiments (training \u2192 evaluation \u2192 ablation)\n- **Resource Management**: Handle GPU memory, checkpoint loading/saving, and data pipeline coordination\n- **Error Handling**: Robust error handling for long-running experimental workflows\n- **Progress Tracking**: Comprehensive logging and progress monitoring across all experiments\n\n## Class Design: ExperimentRunner\n\n### Constructor Logic\n```python\ndef __init__(self, config: Config):\n    \"\"\"\n    Initialize ExperimentRunner with configuration.\n    \n    Logic Flow:\n    1. Store configuration reference for all experiments\n    2. Initialize logging and result storage directories\n    3. Set up device management (GPU/CPU) based on config.system.device\n    4. Initialize shared components that will be reused across experiments:\n       - Tokenizer for Qwen2-1.5B model\n       - Base model architecture (without loading weights initially)\n    5. Create result storage structure for different experiment types\n    6. Set random seeds for reproducibility across all experiments\n    \"\"\"\n```\n\n### Core Experimental Methods\n\n#### 1. Training Experiment (`run_training_experiment`)\n**Purpose**: Execute the main training pipeline described in Section 4.3\n\n**Logic Flow**:\n1. **Dataset Preparation**:\n   - Initialize `DatasetGenerator` with `config.data.procedural_dataset`\n   - Generate 1M procedural CAD sequences if not already available\n   - Create training/validation splits from procedural data\n   - Initialize `DatasetLoader` with appropriate tokenizer\n\n2. **Model Initialization**:\n   - Create `CADRecodeModel` with `config.model` parameters\n   - Initialize point cloud projector with embedding_dim=1536, num_points=256\n   - Load pre-trained Qwen2-1.5B and prepare for fine-tuning\n\n3. **Training Setup**:\n   - Initialize `Trainer` with model, data loaders, and `config.training`\n   - Configure AdamW optimizer (lr=0.0002, weight_decay=0.01)\n   - Set up cosine learning rate scheduler with 1k warmup steps\n\n4. **Training Execution**:\n   - Execute training for 100k iterations with batch_size=18\n   - Monitor training progress and validation metrics\n   - Save checkpoints at regular intervals (config.logging.save_interval)\n   - Log training metrics to wandb if configured\n\n5. **Post-Training**:\n   - Save final model checkpoint\n   - Log training completion metrics\n   - Return training results and model path\n\n#### 2. Evaluation Experiment (`run_evaluation_experiment`)\n**Purpose**: Reproduce Table 1 and Table 2 results from the paper\n\n**Logic Flow**:\n1. **Model Loading**:\n   - Load trained CAD-Recode model from checkpoint\n   - Ensure model is in evaluation mode\n\n2. **Dataset Preparation**:\n   - Initialize test datasets: DeepCAD (8046), Fusion360 (1725), CC3D (2973)\n   - Create appropriate data loaders for each test dataset\n   - Handle different input formats (meshes vs point clouds for CC3D)\n\n3. **Evaluation Execution**:\n   - Initialize `Evaluator` with model and test data loaders\n   - For each dataset:\n     - Run evaluation with test-time sampling (10 candidates)\n     - Compute metrics: Chamfer Distance, IoU, Invalidity Ratio\n     - Collect qualitative results for visualization\n   - Aggregate results across all datasets\n\n4. **Result Processing**:\n   - Format results to match paper tables (Table 1, Table 2)\n   - Generate comparison with baseline methods\n   - Save quantitative results and qualitative examples\n   - Create visualizations for paper figures\n\n#### 3. Ablation Studies (`run_ablation_studies`)\n**Purpose**: Reproduce ablation results from Table 3 and Table 4\n\n**Logic Flow**:\n1. **Training Data Ablation** (Table 3):\n   - Compare DeepCAD dataset (160k) vs procedural dataset (160k, 1M)\n   - For each configuration:\n     - Train model with specific dataset\n     - Evaluate on all test datasets\n     - Record performance metrics\n   - Compare with/without test-time sampling\n\n2. **Architecture Ablation** (Table 4):\n   - Test different point cloud sizes: [64, 128, 256]\n   - Test different model sizes: [Qwen2-0.5B, Qwen2-1.5B]\n   - For each combination:\n     - Initialize model with specific configuration\n     - Train on 1M procedural dataset\n     - Evaluate performance\n   - Identify optimal configuration\n\n3. **Test-Time Sampling Ablation**:\n   - Compare single prediction vs 10-candidate sampling\n   - Analyze impact on Invalidity Ratio specifically\n   - Document computational overhead\n\n4. **Result Analysis**:\n   - Create comprehensive ablation tables\n   - Analyze trends and optimal configurations\n   - Generate insights about model design choices\n\n#### 4. CAD-QA Experiment (`run_cad_qa_experiment`)\n**Purpose**: Reproduce Table 5 results and demonstrate interpretability\n\n**Logic Flow**:\n1. **SGP-Bench Setup**:\n   - Load 1000 CAD-specific questions from SGP-Bench\n   - Convert questions from DeepCAD format to point cloud input format\n   - Prepare GPT-4o integration for answer generation\n\n2. **Baseline Comparisons**:\n   - **CAD-SIGNet Pipeline**:\n     - Generate CAD sequences using CAD-SIGNet\n     - Pass sequences + questions to GPT-4o with interpretation hints\n     - Record accuracy\n   - **PointLLM Pipeline**:\n     - Direct point cloud to answer generation\n     - Record accuracy for comparison\n\n3. **CAD-Recode Pipeline**:\n   - For each question:\n     - Generate CadQuery code from point cloud using CAD-Recode\n     - Pass code + question to GPT-4o (no interpretation hints needed)\n     - Collect and validate answers\n   - Calculate overall accuracy\n\n4. **Interpretability Analysis**:\n   - Demonstrate code interpretability with qualitative examples\n   - Show editing pipeline integration with GPT-4o\n   - Generate interactive editing examples (Figure 6 reproduction)\n\n#### 5. Editing Pipeline Demonstration (`run_editing_experiment`)\n**Purpose**: Demonstrate Figure 6 functionality\n\n**Logic Flow**:\n1. **Code Generation**:\n   - Select representative point clouds\n   - Generate CadQuery code using CAD-Recode\n\n2. **GPT-4o Integration**:\n   - Send code to GPT-4o with editing prompt\n   - Generate refactored code with parameters and sliders\n   - Validate executable code with ipywidgets\n\n3. **Interactive Interface**:\n   - Create Jupyter notebook examples\n   - Demonstrate parameter manipulation\n   - Show real-time CAD model updates\n\n### Utility Methods\n\n#### `setup_experiment_environment`\n**Purpose**: Prepare environment for specific experiment type\n\n**Logic**:\n1. Clear GPU memory if needed\n2. Set appropriate random seeds\n3. Configure logging for experiment type\n4. Create result directories\n5. Initialize monitoring tools (wandb, tensorboard)\n\n#### `save_experiment_results`\n**Purpose**: Standardize result saving across all experiments\n\n**Logic**:\n1. Create timestamped result directories\n2. Save quantitative metrics in structured format (JSON/CSV)\n3. Save model checkpoints with metadata\n4. Generate result summaries and comparisons\n5. Create visualization plots and tables\n\n#### `load_baseline_results`\n**Purpose**: Load baseline method results for comparison\n\n**Logic**:\n1. Load pre-computed baseline results from literature\n2. Format for consistent comparison with CAD-Recode results\n3. Handle missing baselines gracefully\n4. Ensure metric compatibility (same evaluation protocols)\n\n#### `generate_comparison_tables`\n**Purpose**: Create publication-ready result tables\n\n**Logic**:\n1. Format results to match paper table styles\n2. Include statistical significance testing where appropriate\n3. Generate LaTeX table code for publication\n4. Create both detailed and summary comparison views\n\n### Error Handling and Robustness\n\n#### Checkpoint Recovery\n- **Logic**: Detect incomplete experiments and resume from last valid checkpoint\n- **Implementation**: Save experiment state at key milestones\n- **Recovery**: Automatic restart capability for long-running experiments\n\n#### Resource Management\n- **GPU Memory**: Monitor and clear GPU memory between experiments\n- **Storage**: Manage large dataset generation and result storage\n- **Time Limits**: Handle experiment timeouts gracefully\n\n#### Validation Checks\n- **Data Integrity**: Validate dataset completeness before training\n- **Model Consistency**: Verify model architecture matches configuration\n- **Result Validation**: Check result ranges and detect anomalies\n\n### Integration with Configuration\n\n#### Configuration Mapping\n- **Training Config**: Map `config.training` to `Trainer` parameters\n- **Model Config**: Map `config.model` to `CADRecodeModel` initialization\n- **Evaluation Config**: Map `config.evaluation` to `Evaluator` settings\n- **Data Config**: Map `config.data` to dataset generation and loading\n\n#### Experiment Selection\n- **Command Line Interface**: Support selective experiment execution\n- **Configuration Flags**: Enable/disable specific experiment components\n- **Resource Allocation**: Adjust resource usage based on available hardware\n\n### Dependencies and Imports\n\n#### Core Dependencies\n```python\n# Core ML libraries\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Experiment components\nfrom training.trainer import Trainer\nfrom evaluation.evaluator import Evaluator\nfrom data.dataset_generator import DatasetGenerator\nfrom data.dataset_loader import DatasetLoader\nfrom models.cad_recode_model import CADRecodeModel\n\n# Utilities\nfrom config import Config\nimport logging\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\nimport wandb\n```\n\n#### Experiment-Specific Imports\n```python\n# For CAD-QA experiments\nimport openai  # GPT-4o integration\nfrom evaluation.cad_qa import SGPBenchEvaluator\n\n# For visualization and analysis\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# For result processing\nfrom collections import defaultdict\nimport pickle\n```\n\n### Result Structure\n\n#### Experiment Results Format\n```python\nexperiment_results = {\n    \"training\": {\n        \"final_loss\": float,\n        \"training_time\": float,\n        \"model_path\": str,\n        \"training_metrics\": Dict\n    },\n    \"evaluation\": {\n        \"deepcad\": {\"mean_cd\": float, \"median_cd\": float, \"iou\": float, \"ir\": float},\n        \"fusion360\": {\"mean_cd\": float, \"median_cd\": float, \"iou\": float, \"ir\": float},\n        \"cc3d\": {\"mean_cd\": float, \"median_cd\": float, \"iou\": float, \"ir\": float}\n    },\n    \"ablation\": {\n        \"data_ablation\": Dict,\n        \"architecture_ablation\": Dict,\n        \"sampling_ablation\": Dict\n    },\n    \"cad_qa\": {\n        \"accuracy\": float,\n        \"baseline_comparisons\": Dict,\n        \"qualitative_examples\": List\n    }\n}\n```\n\nThis comprehensive logic analysis provides the foundation for implementing a robust experimental framework that faithfully reproduces all experiments described in the CAD-Recode paper while maintaining high code quality and reproducibility standards."}}], "usage": {"input_tokens": 32444, "output_tokens": 2895, "total_tokens": 35339}}]